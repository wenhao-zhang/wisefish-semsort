{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project: Sentiment Analysis Sorting: Summerizing and Sorting Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Group: Wisefish \n",
    "\n",
    "* Wenhao Zhang, wenhaoz \n",
    "* Graeme Milne, gmilne \n",
    "* Mitchell McCormack, mmccorma\n",
    "* Jonathan Lo, jcl60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SemSort Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SemSort utilizes two natural language processing artificial intelligence techniques to automatically aggregate reviews for given business into quickly digestible lists. Review summarization is done using an improved SumBasic algorithm, a multidocument summarization tool. Semantic Analysis is carried out by a neural network trained to predict the positive or negative sentiment of sentence. SemSort is a tool to provide additional subjective information to reviews, offering an extension to Yelpâ€™s star rating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The motivation for SemSort came out of an interest to learn more about the topics of sentiment analysis and text summarization. Our group wanted to test the effectiveness of different models all the while implementing something practical. After viewing the Yelp dataset we came up with the idea of SemSort. Once we confirmed that this was a relatively unique combination of the two NLP areas of study, we decided to to pursue it as our project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Data files used in this project include: \n",
    "* [The Yelp Dataset](https://www.yelp.com/dataset)\n",
    "* [The Opinosis Dataset](https://github.com/kavgan/opinosis/blob/master/OpinosisDataset1.0_0.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code used in this project that was not our own includes: \n",
    "\n",
    "* [rougescore.py](https://github.com/bdusell/rougescore/blob/master/rougescore/rougescore.py)\n",
    "* [LSTM RNN](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Development Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to come up with an idea, the group got together a couple times to brainstorm. We used the provided topics and links as a starting point. After discussing several topics and potential datasets, we eventually settled on using text summarization and sentiment analysis on the yelp reviews dataset. We drew the required pipeline that we would need to follow (see SemSort Pipeline below). After that, we divided up the tasks and set a deadline for ourselves and another meeting date to discuss any final obstacles as well as notebook and poster documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SemSort Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](SemsortpipelineColor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SemSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Semantic Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Sorting function takes in an array of sentences vectorizes them then uses the `model.predict()` function to get a 2 element vector returned for each sentence. The first element in the vector is a float showing the predicted negative sentiment and the second element is the predicted positive sentiment. The sort algorithm then takes the higher of the two values and places the sentence in output dictionary accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "\n",
    "def basicSentimentSort(sentences, with_sentiment=False):\n",
    "    #setup\n",
    "    model = load_model('model5.h5')\n",
    "    # most common words used as features\n",
    "    max_features = 3000\n",
    "    # Tokenizer for preprocessing sentence to wordVecs \n",
    "    tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "    print(\"Sorting: \")\n",
    "    bar = progressbar.ProgressBar(max_value=len(sentences))\n",
    "\n",
    "    sorted_sentences = {\"Positive\":[], \"Negative\":[]}\n",
    "\n",
    "    for i, sentence in enumerate(sentences): \n",
    "        # vectorize the sentence\n",
    "        tokenized_sentence = tokenizer.texts_to_sequences(sentence)\n",
    "        # pad the sentences for the network \n",
    "        tokenized_sentence = pad_sequences(tokenized_sentence, maxlen=944, dtype='int32', value=0)\n",
    "        # generate the sentiment prediction from the model -> 2 element list [negative <float> , positive <float>]\n",
    "        sentiment = model.predict(tokenized_sentence,batch_size=1,verbose = 2)[0]\n",
    "        #place sentence in the output dict\n",
    "        if sentiment[1] > sentiment[0]:\n",
    "            if with_sentiment:\n",
    "                sorted_sentences[\"Positive\"].append((sentence, sentiment))\n",
    "            else:\n",
    "                sorted_sentences[\"Positive\"].append(sentence)  \n",
    "        else:\n",
    "            if with_sentiment:\n",
    "                sorted_sentences[\"Negative\"].append((sentence, sentiment))\n",
    "            else:\n",
    "                sorted_sentences[\"Negative\"].append(sentence)  \n",
    "        bar.update(i)\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "voteSentimentSort was an attemp to leverage three models. The first model was accurate at predicting the positive sentimemnt but not very accurate at predicting the negative sentiment. The Second model was the inverse of hte first model and the third model was fairly accurate for both positive and negative but not as accurate as teh first two mdoels in their respective categories. \n",
    "\n",
    "A sentence is passed the the `model.predict()` for each model then the argmax of the average of hte predicted sentiment is used to place the model in the output dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def voteSentimentSort(sentences):\n",
    "    # good at finding positive sentiment\n",
    "    model1 = load_model('D:\\\\SFU\\\\cmpt-413\\\\nlpclass-1187-g-wisefish\\\\project\\\\yelpModel\\\\model1.h5')\n",
    "    # good at finding negative sentiment\n",
    "    model2 = load_model('D:\\\\SFU\\\\cmpt-413\\\\nlpclass-1187-g-wisefish\\\\project\\\\yelpModel\\\\model2.h5')\n",
    "    model3 = load_model('D:\\\\SFU\\\\cmpt-413\\\\nlpclass-1187-g-wisefish\\\\project\\\\yelpModel\\\\model5.h5')\n",
    "    max_features = 3000\n",
    "    tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "\n",
    "    sentence_sentiment = {}\n",
    "    print(\"\\nPredict with Model 1: \")\n",
    "    bar1 = progressbar.ProgressBar(max_value=len(sentences))\n",
    "    for i, sentence in enumerate(sentences): \n",
    "        #vectorizing \n",
    "        tokenized_sentence = tokenizer.texts_to_sequences(sentence)\n",
    "        #padding \n",
    "        tokenized_sentence = pad_sequences(tokenized_sentence, maxlen=975, dtype='int32', value=0)\n",
    "        sentiment = model1.predict(tokenized_sentence,batch_size=1,verbose = 2)[0]\n",
    "        sentence_sentiment[sentence] = [sentiment[0], sentiment[1]]\n",
    "        bar1.update(i)\n",
    "    print(\"\\nPredict With Model 2: \")\n",
    "    bar2 = progressbar.ProgressBar(max_value=len(sentences))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        #vectorizing \n",
    "        tokenized_sentence = tokenizer.texts_to_sequences(sentence)\n",
    "        #padding \n",
    "        tokenized_sentence = pad_sequences(tokenized_sentence, maxlen=273, dtype='int32', value=0)\n",
    "        sentiment = model2.predict(tokenized_sentence,batch_size=1,verbose = 2)[0]\n",
    "        sentence_sentiment[sentence].append(sentiment[0])\n",
    "        sentence_sentiment[sentence].append(sentiment[1])\n",
    "        bar2.update(i)\n",
    "    \n",
    "    print(\"\\nPredict With Model 3: \")\n",
    "    bar3 = progressbar.ProgressBar(max_value=len(sentences))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        #vectorizing \n",
    "        tokenized_sentence = tokenizer.texts_to_sequences(sentence)\n",
    "        #padding \n",
    "        tokenized_sentence = pad_sequences(tokenized_sentence, maxlen=946, dtype='int32', value=0)\n",
    "        sentiment = model3.predict(tokenized_sentence,batch_size=1,verbose = 2)[0]\n",
    "        sentence_sentiment[sentence].append(sentiment[0])\n",
    "        sentence_sentiment[sentence].append(sentiment[1])\n",
    "        bar3.update(i)\n",
    " \n",
    "    print(\"\\nSorting: \")\n",
    "    sorted_sentences = {\"Positive\":[], \"Negative\":[]}\n",
    "    for sentence, sentiment in sentence_sentiment.items():\n",
    "        print(sentiment)\n",
    "        m1_neg = sentiment[0]\n",
    "        m1_pos = sentiment[1]\n",
    "        m2_neg = sentiment[2]\n",
    "        m2_pos = sentiment[3]\n",
    "        m3_neg = sentiment[4]\n",
    "        m3_pos = sentiment[5]\n",
    "\n",
    "        avg_pos =  (m1_pos + m2_pos + m3_pos) / 3\n",
    "        avg_neg =  (m1_neg + m2_neg + m3_neg) / 3 \n",
    "        \n",
    "        if avg_neg < avg_pos: \n",
    "            sorted_sentences[\"Positive\"].append(sentence)\n",
    "        else:\n",
    "            sorted_sentences[\"Negative\"].append(sentence)\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Full Pipeline Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Two SemSorts were implemented. The first is a single path algorithm that will summarize reviews then sort the summarized sentences into positive and negative categories and return the sorted summarized sentences. The multipath algorithm will split up the reviews based upon the start rating then run the single path on the lowest rated reviews and the highest rated reviews before merging them and taking the top *n* sentences to return. \n",
    "\n",
    "The multipath algorithm was implemented to account for the case where positive sentences existed in low star reviews or the inverse of negative sentences existing in high star reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sum_basic import SumBasic\n",
    "from yelpModel.sentimentSort import basicSentimentSort,voteSentimentSort\n",
    "import pandas as pd\n",
    "\n",
    "def pickTop(n, reviews, idx):\n",
    "    sorted_reviews = sorted(reviews,  key=lambda tup: tup[1][idx],reverse=True)\n",
    "    top = []\n",
    "    for i in range(n):\n",
    "        if i < len(sorted_reviews):            \n",
    "            top.append(sorted_reviews[i][0])\n",
    "    return top\n",
    "\n",
    "def SemSort(data, multipath=False, num_bullet_points=4):\n",
    "    sorted = {\"Positive\":[], \"Negative\":[]}\n",
    "    if multipath:\n",
    "        \n",
    "        #Split Reviews by star rating\n",
    "        oneStar = []\n",
    "        twoStar = []\n",
    "        fourStar = []\n",
    "        fiveStar = []\n",
    "        for _, row in data.iterrows():\n",
    "            if row[\"Stars\"] == 1:\n",
    "                oneStar.append(row[\"Text\"])\n",
    "            elif row[\"Stars\"] == 2:\n",
    "                twoStar.append(row[\"Text\"])\n",
    "            elif row[\"Stars\"] == 4:\n",
    "                fourStar.append(row[\"Text\"])\n",
    "            elif row[\"Stars\"] == 5:\n",
    "                fiveStar.append(row[\"Text\"])\n",
    "        \n",
    "        low_reviews = oneStar + twoStar\n",
    "        high_reviews = fiveStar + fourStar\n",
    "        \n",
    "        #Summarize low start reviews\n",
    "        neg_summarizer = SumBasic(low_reviews)\n",
    "        neg_summary = neg_summarizer.get_summary(10)\n",
    "        # Sort low star reviews\n",
    "        neg_sorted = basicSentimentSort(neg_summary, with_sentiment=True)\n",
    "        #Summarize high star reviews\n",
    "        pos_summarizer = SumBasic(high_reviews)\n",
    "        pos_summary = pos_summarizer.get_summary(10)\n",
    "        #Sort high star reviews\n",
    "        pos_sorted = basicSentimentSort(pos_summary, with_sentiment=True)\n",
    "\n",
    "        total_pos = pos_sorted[\"Positive\"] + neg_sorted[\"Positive\"]\n",
    "        total_neg = pos_sorted[\"Negative\"] + neg_sorted[\"Negative\"]\n",
    "        \n",
    "        #Merge the reviews by taking the top n from the positive and negative sentences\n",
    "        sorted[\"Positive\"] = pickTop(num_bullet_points, total_pos, 1)\n",
    "        sorted[\"Negative\"] = pickTop(num_bullet_points, total_neg, 0)\n",
    "            \n",
    "\n",
    "    else:\n",
    "        # collect reviews into list of strings\n",
    "        data_to_be_sorted = []\n",
    "        for _, row in data.iterrows():\n",
    "            data_to_be_sorted.append(row[\"Text\"])\n",
    "\n",
    "        #generate summaries of the reviews\n",
    "        summarizer = SumBasic(data_to_be_sorted)\n",
    "        summary = summarizer.get_summary(num_bullet_points)\n",
    "        #sort summaries with RNN preditctions\n",
    "        sorted = basicSentimentSort(summary)\n",
    "\n",
    "    return sorted \n",
    "csv_data = pd.read_csv('CF_Toronto_Eaton_Centre_reviews.csv')\n",
    "\n",
    "sorted = SemSort(csv_data, multipath=False, num_bullet_points=10)\n",
    "\n",
    "print(\"\\nPositive\")\n",
    "for i, sentence in enumerate(sorted[\"Positive\"]):\n",
    "    print(f'{i+1}: {sentence}')\n",
    "\n",
    "print(\"\\nNegative\")\n",
    "for i, sentence in enumerate(sorted[\"Negative\"]):\n",
    "     print(f'{i+1}: {sentence}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are using the Yelp Academic DataSet (https://www.yelp.com/dataset/challenge) as our primary data source for reviews.\n",
    "This dataset contains around 188,000 businesses and 6,000,000 reviews that were featured in Yelp at the time of collection.\n",
    "\n",
    "For our evaluation purposes, we are only wanting to look at a subset of businesses and a subset of review data, namely the number of stars and the review's text content:\n",
    "1. Load in the businesses listings and filter to only those in Toronto, Ontario, Canada with over 100 reviews.\n",
    "2. Load in the review data, filter to only those matching the above businesses, and keep only the star rating and text content.\n",
    "3. Retrieve all the review text contents for each business and feed it into the SumBasic and SemSort components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing input for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class used for preprocessing the input for summarization is called ProcessData. The initialization function takes in 6 inputs. The first input variable data, is the data that needs to be processed. The type_pos_tag variable determines the type of POS tagger to be used. The default input is NLTK, this used the built in pos tagger from nltk. Setting this variable to Stanford will switch to using the Stanford pos tagger. The remove_short feature removes sentences that are too short. The lemmatize variable enables or disables lemmatization. True will use lemmatization. Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed together as a single unit. We found this to help the quality of summarization from SumBasic. The remove_stop_word variable will cause stop words to be removed if it is set to true. Stop words are those words that do not provide much information. In our case we justed the NLTK english stop words. The remove_punc variable will remove all punctuations if it is set to true. This was helpful for SumBasic as we found if we did not remove the punctuation, they would skew the results. Some of the reviews on Yelp were just some punctuations.\n",
    "\n",
    "The first part of the initialization is to remove the short sentences. This removes any sentences that are less than length of 2. In the next step we tokenize the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def __init__(self, data, type_pos_tag = \"NLTK\", remove_short = True, lemmatize = True, remove_stop_word = True, remove_punc = True):\n",
    "\n",
    "        self.puncs = [\".\",\",\",\";\",\":\",\"!\",\"?\",\"``\",\"''\",\"'\",\"-\"]\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "        self.sentences = []\n",
    "        self.type_pos_tag = type_pos_tag\n",
    "        self.pos_tagger = StanfordPOSTagger('./stanford/models/english-bidirectional-distsim.tagger','./stanford/stanford-postagger.jar')\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_stop_word = remove_stop_word\n",
    "        self.remove_punc = remove_punc\n",
    "\n",
    "        if remove_short:\n",
    "            for i, line in enumerate(data):\n",
    "                if len(line) < 2:\n",
    "                    del data[i]\n",
    "\n",
    "        for line in data:\n",
    "            tokens = sent_tokenize(line)\n",
    "            for s in tokens:\n",
    "                if s not in self.sentences:\n",
    "                    self.sentences.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to remove any punctuations. It removes any punctuation in the following list: [\".\",\",\",\";\",\":\",\"!\",\"?\",\"``\",\"''\",\"'\",\"-\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _remove_punc(self, sent):\n",
    "        \n",
    "        for punc in self.puncs:\n",
    "            sent = list(filter(lambda a: a[0] != punc, sent))\n",
    "       \n",
    "        return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts a treebank tag to wordnet tag. This is needed as the lemmatizer from NLTK takes in a wordnet tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def _get_wordnet_pos(self, treebank_tag):\n",
    "        switch = {\"J\": wordnet.ADJ, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return switch.get(treebank_tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans the inputs for summarization. The function first tokenizes the words in the sentences. It then does pos tagging based on what pos tagger is being used. It then lower cases the words so they will be treated the same. Then the puncuations are removed. Next the top words are remove and lemmatization is performed if it is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentences(self):\n",
    "        cleaned = []\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "\n",
    "            #words = [w.lower() for w in words]\n",
    "\n",
    "            #pos tagging\n",
    "            if self.type_pos_tag == \"Stanford\":\n",
    "                tokens = []\n",
    "                for w in words:\n",
    "                    tokens.extend(self.pos_tagger.tag([w]))\n",
    "\n",
    "            else:\n",
    "                tokens = []\n",
    "                for w in words:\n",
    "                    tokens.extend(pos_tag([w]))\n",
    "\n",
    "            tokens = [(t[0].lower(),t[1]) for t in tokens]\n",
    "\n",
    "            #remove punctuations\n",
    "            if self.remove_punc:\n",
    "                tokens = self._remove_punc(tokens)\n",
    "\n",
    "            #lemmatize\n",
    "            if self.lemmatize:\n",
    "                tokens = [(self.lemmatizer.lemmatize(t[0], self._get_wordnet_pos(t[1])),t[1]) for t in tokens]\n",
    "\n",
    "            #remove stop words\n",
    "            if self.remove_stop_word:\n",
    "                tokens = [t for t in tokens if t[0] not in self.stopwords]\n",
    "\n",
    "            cleaned.append(tokens)\n",
    "        \n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function removes the pos tags from the data set as the clean_sentences function returns a list of tuples containing the word and its pos tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def remove_tags(self, data):\n",
    "        cleaned = []\n",
    "\n",
    "        for s in data:\n",
    "            temp = [w[0] for w in s]\n",
    "            cleaned.append(temp)\n",
    "        \n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SumBasic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "SumBasic is a summerization system for multi document input that uses word probability to determine the importance of sentences. Each sentence is assigned a weight equal to the average probability of the words in it. Then the best scoring sentence is selected. The algorithm runs until the generated summary meets the expected length. The full steps to the SumBasic algorithm are shown below.\n",
    "\n",
    "$$\n",
    "\\textbf{Step 1}\\\\\n",
    "\\text{Compute the probability distribution over the words \\(w_i\\) appearing in the input, \\(p(w_i)\\) for every \\(i\\); \\(p(w_i)\\) = \\(\\frac{n}{N}\\), where \\(n\\) is the number of times the word appeared in the input, and \\(n\\) is the total number of content word tokens in the input.}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\textbf{Step 2}\\\\ \n",
    "\\text{For each sentence SJ in the input, assign a weight equal to the average probability of the words in the sentence, i.e.}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "weight (S_j) = \\sum_{w_i) \\in{S_j}} \\frac{p(w_i}{|\\{w_i|w_i\\in{S_j}\\}|} \\\\ \n",
    "$$\n",
    "\n",
    "$$\\textbf{Step 3}\\\\ \\text{Pick the best scoring sentence that contains the highest probability word.}\\\\ \n",
    "$$\n",
    "\n",
    "$$ \\textbf{Step 4}\\\\ \\text{For each word \\(w_i\\) in the sentence chosen at step 3, update their probability} \n",
    "$$ \n",
    "\n",
    "$$ p_{new}(w_i) = p_{old}(w_i) * p_{old}(w_i)\\\\ $$\n",
    "$$ \\textbf{Step 5}\\\\ \\text{If the desired summary length has not been reached, go back to Step 2.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class for performing SumBasic summarization is called SumBasic. This class takes in one input, the data to be summarized. In the init function the class creates a copy of the data to be later used to output the summaries. The sentences are then sent to the data processing class to be prepared for use in the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, data):\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in data:\n",
    "            tokens = sent_tokenize(sentence)\n",
    "            for s in tokens:\n",
    "                if s not in self.data:\n",
    "                    self.data.append(tokens)\n",
    "        \n",
    "        self.sentence_weights = {}\n",
    "\n",
    "        data_processor = ProcessData(data)\n",
    "        \n",
    "        self.sentences = data_processor.remove_tags(data_processor.clean_sentences())\n",
    "\n",
    "        self.probabilities = self._get_probabilities(self.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function updates the probability of each word in the sentence that won. The score updated with the old probability of the word squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _update_probabilities(self, winner):\n",
    "        for word in winner:\n",
    "            self.probabilities[word.lower()] = self.probabilities[word.lower()] * self.probabilities[word.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get probability function intiailizes the probabilities for the SumBasic summarization. The program counts the number of times that each words appears in all the sentences. Then it divides the count of each individual word by the total number of tokens in the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_probabilities(self, data):\n",
    "        \n",
    "        word_probabilities = {}\n",
    "        token_count = 0\n",
    "\n",
    "        for sentence in data:\n",
    "\n",
    "            for word in sentence:\n",
    "                token_count += 1\n",
    "\n",
    "                if word in word_probabilities:\n",
    "                    word_probabilities[word.lower()] += 1\n",
    "                else:\n",
    "                    word_probabilities[word.lower()] = 1\n",
    "\n",
    "        for word in word_probabilities:\n",
    "            word_probabilities[word] = word_probabilities[word] / token_count\n",
    "\n",
    "        return word_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _weight_sentence function get the weight of the sentence being looked at. The sentence weight is defined as the sum of the weight of all the probabilites of each word in the sentence divided by the total amount of words in the sentence. If the token_count is zero, the function will return 0 for the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def _weight_sentence(self, sentence):\n",
    "        sentence_count = 0\n",
    "        token_count = 0\n",
    "        for word in sentence:\n",
    "            if word in self.probabilities:\n",
    "                sentence_count += self.probabilities[word]\n",
    "                token_count += 1\n",
    "        \n",
    "        return sentence_count/token_count if token_count > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will look throught all the sentences and find those that contain the highest probability word and return the most likely sentence. It first finds the most likely word. It then finds all the sentences that contain the most likely word. It then looks through the candidates and determines the sentence with the highest sentence weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_max_sentence(self):\n",
    "        highest_prob_word = max(self.probabilities.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "        sentences_containing_highest_prob_word = []\n",
    "        for sentence in self.sentences:\n",
    "            if highest_prob_word in sentence:\n",
    "                sentences_containing_highest_prob_word.append(sentence)\n",
    "\n",
    "        winner = \"\"\n",
    "        winner_prob = 0\n",
    "\n",
    "        for sentence in sentences_containing_highest_prob_word:\n",
    "            if self.sentence_weights[tuple(sentence)] > winner_prob:\n",
    "                winner_prob = self.sentence_weights[tuple(sentence)]\n",
    "                winner = sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to get the summary. The function loops through the data until a desired about of sentences for the summary is reached or there is no more data to be processed. For each of the sentences, it gets the weight of the sentence for the current iteration. It then gets the sentence with max weight. Then it adds the sentence to the summary and repeats until the desire length is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    def get_summary(self, length):\n",
    "        \n",
    "        summary = []\n",
    "\n",
    "        while len(summary) < length and len(self.data) > 0:\n",
    "            for sentence in self.sentences:\n",
    "                self.sentence_weights[tuple(sentence)] = self._weight_sentence(sentence)\n",
    "\n",
    "            winner = self._get_max_sentence()\n",
    "            winner_index  = self.sentences.index(winner)\n",
    "\n",
    "            summary.append(self.data[winner_index][0])\n",
    "\n",
    "            if winner != '':\n",
    "                self._update_probabilities(winner)\n",
    "                del self.sentences[winner_index]\n",
    "                del self.data[winner_index]\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinosis Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpinosisGraph class generates the graph that the OpinosisSummarizer uses. It takes in the data and two other inputs. These tell the class to use lemmatization or to remove the stop words. The first thing the init function does is call the ProcessData class to get the data preprocessed for summarizatoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __init__(self, data, remove_stop_word = False, lemmatize = True):\n",
    "        self.sentences = []\n",
    "        self.graph = {}\n",
    "        self.PRI = {}\n",
    "        processor =  ProcessData(data,\"Stanford\", False, lemmatize, remove_stop_word, False)\n",
    "\n",
    "        self.sentences = processor.clean_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates the graph. The function loops through each sentence and adds each word in the sentence to the graph. Each node of the graph contains a list of PRI, Positional Reference Information, and the word with its pos tag. The SID of a word is the sentence identifier, or the sentence it appears in. PID is the position of occurence of the location in the sentence it appeared in. Together this forms a PRI. If the node exists, the list of PRI is just updated with the new location. If it does not exist a new node is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def generate_opinosis_graph(self):\n",
    "        n = len(self.sentences)\n",
    "        for i in range(n):\n",
    "            words =  self.sentences[i]\n",
    "            \n",
    "            for (j,(word,pos)) in enumerate(words):\n",
    "                if word == \"'s\" and pos == \"VBZ\":\n",
    "                    words[j] = (\"is\", pos)\n",
    "                if word == \"n't\" and pos == \"RB\":\n",
    "                    words[j] = (\"not\", pos)\n",
    "                if word == \"wa\" and pos == \"VBD\":\n",
    "                    words[j] = (\"was\", pos)\n",
    "                if word == \"ca\" and pos == \"MD\":\n",
    "                    words[j] = (\"can\", pos)\n",
    "\n",
    "            sent_size = len(words)\n",
    "            for j in range(sent_size):\n",
    "                LABEL = words[j]\n",
    "                PID = j\n",
    "                SID = i\n",
    "                if LABEL in self.PRI:\n",
    "                    self.PRI[LABEL].append((SID, PID))\n",
    "                else:\n",
    "                    self.graph[LABEL] = []\n",
    "                    self.PRI[LABEL] = [(SID, PID)]\n",
    "                \n",
    "                prev_node = words[j-1]\n",
    "\n",
    "                if not self._edge_exists(prev_node, LABEL) and j > 0:\n",
    "                    self.graph[prev_node].append(LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if the edge between the previous node and current node already exists. If it exists a new node will not be created. Since the graph is based on a dictionary, the function gets the list of nodes from the prev_node and check if the current is in the list. If it is, it returns True otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _edge_exists(self, prev_node, curr_node):\n",
    "        if prev_node in self.graph:\n",
    "            edges = self.graph[prev_node]\n",
    "            if curr_node in edges:\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Opinosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Opinosis is an abstractive summarization method that makes use of graphs to generate summaries of highly redundant opinions. Unlike many other methods Opinosis does not require any domain knowledge and only uses shallow NLP processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class for Opinosis summarization is called OpinosisSummarizer. The first thing the summarizer gets its the graph for the summary from the OpinosisGraphc class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, data, sigma_ss, sigma_r, sigma_gap, sigma_vsn, collapse, similarity, remove_stop_word=False, lemmatize=True):\n",
    "        self.sigma_ss = sigma_ss\n",
    "        self.sigma_r = sigma_r\n",
    "        self.sigma_gap = sigma_gap\n",
    "        self.sigma_vsn = sigma_vsn\n",
    "        self.collapse = collapse\n",
    "        self.similarity = similarity\n",
    "\n",
    "        graph = OpinosisGraph(data, remove_stop_word, lemmatize)\n",
    "        graph.generate_opinosis_graph()\n",
    "\n",
    "        self.graph = graph.graph\n",
    "        self.PRI = graph.PRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines if a node is a valid start node. A node is a valid start location of the average of the PID of the node are less than or equal a parameter that is an user input, sigma_vsn. If it is less than or equal to sigma_vsn, the node must also be a natural starting point. This include words like its, the, when, a, an, this, the, they, it ,i, we, and our. A valid start pos tag is JJ, RB, PRP$, VBG, NN, and DT. If all of the conditions are met, the node is a valid start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def _is_valid_start_node(self, node):\n",
    "        PRI_node = self.PRI[node]\n",
    "\n",
    "        common_start_words = \"r^(its/|the/|when/|a/|an/|this/|the/|they/|it/|i/|we/|our/).*\"\n",
    "\n",
    "        valid_start_tags = [\"/JJ\", \"/RB\", \"/PRP$\", \"/VBG\", \"/NN\", \"/DT\"]\n",
    "\n",
    "        len_PRI = len(PRI_node)\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        for PRI in PRI_node:\n",
    "            total += PRI[1]\n",
    "\n",
    "        word_tag = node[0]+\"/\"+node[1]\n",
    "\n",
    "        if total/len_PRI <= self.sigma_vsn:\n",
    "            if re.match(common_start_words, word_tag, re.I) or \"it/PRP\" in word_tag or \"if/\" in word_tag or \"for/\" in word_tag:\n",
    "                return True\n",
    "            for v_t in valid_start_tags:\n",
    "                if v_t in word_tag:\n",
    "                    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines if the node is a valid end node. It is a valid end if the node ahs a pos tag of ., ,, or CC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    def _is_valid_end_node(self, node):\n",
    "        if node[1] == \".\" or node[1] == \",\" or node[1] == \"CC\":\n",
    "            return True\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folloing function determines if the path is a valid path. It is a valid path if it satisfies one of the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _is_valid_path(self, sentence):\n",
    "\n",
    "        well_formed = False\n",
    "\n",
    "        pos_sent = \"\"\n",
    "\n",
    "        for w in sentence:\n",
    "            pos_sent += \"/\" + w[1]\n",
    "\n",
    "        if re.match(\".*(/JJ)*.*(/NN)+.*(/VB)+.*(/JJ)+.*\", pos_sent, re.I):\n",
    "            well_formed = True\n",
    "        elif not re.match(\".*(/DT).*\", pos_sent, re.I) and re.match(\".*(/RB)*.*(/JJ)+.*(/NN)+.*\", pos_sent, re.I):\n",
    "            well_formed = True\n",
    "        elif re.match(\".*(/PRP|/DT)+.*(/VB)+.*(/RB|/JJ)+.*(/NN)+.*\", pos_sent, re.I):\n",
    "            well_formed = True\n",
    "        elif re.match(\".*(/JJ)+.*(/TO)+.*(/TO)+.*(/VB).*\", pos_sent, re.I):\n",
    "            well_formed = True\n",
    "        elif re.match(\".*(/RB)+.*(/IN)+.*(/NN)+.*\", pos_sent, re.I):\n",
    "            well_formed = True\n",
    "\n",
    "        last = sentence[-1][1]\n",
    "\n",
    "        #if re.match(\"(/TO|/VBZ|/IN|/CC|/PRP|/DT|/,)\", last, re.I):\n",
    "        #    well_formed = False\n",
    "\n",
    "        return well_formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does scoring for each incremental step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def _Swt_loglen(self, redudancy, L):\n",
    "        return math.log(L, 2)*redudancy if L > 1 else redudancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if a node is a hub node that can be a candidate for collapsing. A node is collapsible if it is a VB, this is because verbs are common in reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _is_collapsible(self, node):\n",
    "        return bool(re.match(r\"VB\", node[1], re.I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function find the intersection of sids such that the pids are less than a value sigma_gap. This determines if a path is redundant. The sigma_gap value determines the maximum gap that can be allowed to finding redundant paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def _intersect(self, PRI_n, PRI_overlap):\n",
    "        PRI_intersect = []\n",
    "        for sid_o, pid_o in PRI_overlap:\n",
    "            for sid, pid in PRI_n:\n",
    "                if sid_o == sid:\n",
    "                    if pid > pid_o and abs(pid - pid_o) <= self.sigma_gap:\n",
    "                        PRI_intersect.extend([(sid, pid)])\n",
    "                        break\n",
    "                else:\n",
    "                    if sid > sid_o:\n",
    "                        break\n",
    "\n",
    "        return PRI_intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines the Jaccard similarity of the two sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _jaccard_similarity(self, candidate1, candidate2):\n",
    "\n",
    "        if set(candidate1).issubset(set(candidate2)) or set(candidate2).issubset(set(candidate1)):\n",
    "            return 1\n",
    "\n",
    "        intersection_size = len(set(candidate1).intersection(set(candidate2)))\n",
    "        union_size = len(set(candidate1).union(set(candidate2)))\n",
    "        return intersection_size/union_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to remove the duplicates of the list of candidates. These duplicates are near duplicates. This means that they are very close to each other. The algorithm used in this duplicate removal function is similar to an agglomerative clustering algorithm. First the function calculates the Jaccard index between half the pairs of the sentences. The Jaccard index is used to determine if the sentences are near duplicates. Next the pair with the highest Jaccard index is found. If the value is greater than similarity, they are clustered together. The algorithm then repeats the process until there is no change in the number of clusters. After the function finds the sentence with the highest score for each cluster. These sentences are returned as the final candidates without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def _remove_duplicates(self, candidates):\n",
    "\n",
    "        clusters = [[key] for key, value in candidates.items()]\n",
    "        final_sentences = []\n",
    "\n",
    "        prev_size = len(clusters)\n",
    "        curr_size = prev_size * 2\n",
    "        while curr_size != prev_size and len(clusters) > 1:\n",
    "\n",
    "            prev_size = len(clusters)\n",
    "\n",
    "            scores = []\n",
    "            temp = []\n",
    "\n",
    "            size_cluster = len(clusters)\n",
    "\n",
    "            for i in range(size_cluster):\n",
    "\n",
    "                c1 = clusters[i]\n",
    "\n",
    "                for j in range(i + 1, size_cluster):\n",
    "\n",
    "                    c2 = clusters[j]\n",
    "                    best_score = self._jaccard_similarity(c1[0], c2[0])\n",
    "\n",
    "                    for a in c1:\n",
    "                        for b in c2:\n",
    "                            score = self._jaccard_similarity(a, b)\n",
    "                            if score > best_score:\n",
    "                                best_score = score\n",
    "\n",
    "                    key = c1+c2\n",
    "                    temp.append([c1, c2])\n",
    "                    scores.append((key, best_score))\n",
    "\n",
    "            highest = max(scores, key=lambda x: x[1])\n",
    "            highest_index = scores.index(highest)\n",
    "\n",
    "            if highest[1] > self.similarity:\n",
    "                clusters.append(highest[0])\n",
    "\n",
    "                for i in temp[highest_index]:\n",
    "                    clusters.remove(i)\n",
    "\n",
    "            curr_size = len(clusters)\n",
    "\n",
    "        for cluster in clusters:\n",
    "            max_sentence = max(cluster, key=candidates.get)\n",
    "            final_sentences.append(tuple(max_sentence))\n",
    "\n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to calculate the path score for a group of collapsed candidates. It is the average of all the scores of the ccs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _average_path_score(self, cc, score):\n",
    "\n",
    "        len_cc = len(cc)\n",
    "\n",
    "        scores = 0\n",
    "\n",
    "        for i in cc:\n",
    "            scores += score[i]\n",
    "\n",
    "        return scores/len_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to stitch together all the collapsed candidates. It first appends all the cc except the last one with commas. For the laste one, a coordinating conjuction is needed. It looks through all the coordinating conjuctions and determines the one with the highest redundancy. The winner is used to connect the last cc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   def _stitch(self, anchor, cc):\n",
    "        new_sent = anchor[:]\n",
    "\n",
    "        if len(cc) > 1:\n",
    "            for s in cc[:-1]:\n",
    "                for w in s:\n",
    "                    new_sent += (w,)\n",
    "                new_sent += ((\",\", \",\"),)\n",
    "            last_node = cc[-1][0]\n",
    "\n",
    "            cc_nodes = [(\"and\", \"CC\"), (\"for\", \"IN\"), ('nor', 'CC'),\n",
    "                        ('but', 'CC'), ('or', 'CC'), ('yet', 'RB'), ('so', 'RB')]\n",
    "\n",
    "            highest_r = 0\n",
    "            best_cc = cc_nodes[0]\n",
    "\n",
    "            for node in cc_nodes:\n",
    "                try:\n",
    "                    r = len(self._intersect(\n",
    "                        self.PRI[node], self.PRI[last_node]))\n",
    "                    if r > highest_r:\n",
    "                        highest_r = r\n",
    "                        best_cc = node\n",
    "                except(KeyError):\n",
    "                    pass\n",
    "\n",
    "            new_sent += (best_cc,)\n",
    "\n",
    "            for w in cc[-1]:\n",
    "                new_sent += (w,)\n",
    "\n",
    "        else:\n",
    "            for w in cc[0]:\n",
    "                new_sent += (w,)\n",
    "\n",
    "        return new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to traverse the nodes to find candidate sentences. If the node is not redundant enough nothing is done. If it is, the function first checks if the the sentence is a valid sentence. If it is, it is added along with its score and the function returns the list of candidates. If not the function will loop through each of the neighbours of the current node. If the neighbour is a candidate for collapse, it will collapse the node, otherwise it will call _traverse to continue looking for candidate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _traverse(self, candidates, node, score, PRI_overlap, sentence, length):\n",
    "        redundancy = len(PRI_overlap)\n",
    "        if redundancy >= self.sigma_r:\n",
    "            if self._is_valid_end_node(node):\n",
    "                if self._is_valid_path(sentence):\n",
    "                    final_score = score/length\n",
    "                    candidates[sentence] = final_score\n",
    "\n",
    "            for v_n in self.graph[node]:\n",
    "                PRI_new = self._intersect(self.PRI[v_n], PRI_overlap)\n",
    "                r = len(PRI_new)\n",
    "                new_sentence = sentence + (v_n,)\n",
    "                L = length + 1\n",
    "                new_score = score + self._Swt_loglen(r, L)\n",
    "                if self._is_collapsible(v_n) and self.collapse:\n",
    "                    c_anchor = new_sentence\n",
    "                    tmp = {}\n",
    "                    for v_x in self.graph[v_n]:\n",
    "                        self._traverse(tmp, v_x, 0, PRI_new, (v_x,), L)\n",
    "\n",
    "                        if tmp:\n",
    "                            cc = self._remove_duplicates(tmp)\n",
    "                            cc_path_score = self._average_path_score(cc, tmp)\n",
    "                            final_score = new_score + cc_path_score\n",
    "                            sitch_sent = self._stitch(c_anchor, cc)\n",
    "                            print(sitch_sent)\n",
    "                            candidates[sitch_sent] = final_score\n",
    "                else:\n",
    "                    self._traverse(candidates, v_n, new_score,\n",
    "                                   PRI_new, new_sentence, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for getting the summary from Opinosis. The function will loop through each node in the graph. It determines if the node is a valid start node, if it is it will call _traverse to try to produce a candidate sentence. Once the _traverse returns the candidates are added to the list of candidates. Once the loop is complete the function removes the duplicate candidates and gets the top sentences. The number returned is determines by sigma_ss. The candidates is turned into sentences and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def get_summary(self):\n",
    "        candidates = {}\n",
    "        summaries = []\n",
    "\n",
    "        for v_j in self.graph.keys():\n",
    "\n",
    "            if self._is_valid_start_node(v_j):\n",
    "\n",
    "                path_length = 1\n",
    "                score = 0\n",
    "                clist = {}\n",
    "                self._traverse(clist, v_j, score,\n",
    "                               self.PRI[v_j], (v_j,), path_length)\n",
    "                candidates.update(clist)\n",
    "\n",
    "        final = self._remove_duplicates(candidates)\n",
    "        top = sorted(final, key = candidates.get, reverse=True)[:self.sigma_ss]\n",
    "        #top = sorted(final, key=candidates.get, reverse=True)\n",
    "\n",
    "        for s in top:\n",
    "            summary = \"\"\n",
    "            length = len(s)\n",
    "            for i in range(length-1):\n",
    "                summary += s[i][0]\n",
    "\n",
    "                if s[i+1][1] == \".\":\n",
    "                    summary += s[i+1][0]\n",
    "                elif (s[i+1][0] == \",\" or s[i+1][0] == \"and\") and i + 1 == length:\n",
    "                    summary += \".\"\n",
    "                elif s[i+1][1] != \",\":\n",
    "                    summary += \" \"\n",
    "\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experimental Setup #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Validation of SumBasic and Opinosis Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to test that our SumBasic and Opinosis implementation are outputting useful data we use the ROUGE evaluation metrics. ROUGE stands for recall-orientated understudy for gisting evaluation and is often used for evaluating machine text summarization and machine translation ( https://en.wikipedia.org/wiki/ROUGE_(metric) ). We specifically look at Rouge-N which measures the overlap in the n-grams between our generated summary and one or more gold standard models in the dataset. The dataset that we used for testing is the [Opinosis dataset](http://kavita-ganesan.com/opinosis-opinion-dataset/#.XAM97BB6ov4) which has an attached research paper with which we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ROGUE in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the ROGUE evaluation we have two items: \n",
    "- A machine generated summary (output of SumBasic) \n",
    "- A set of one or more gold standard reference summaries\n",
    "\n",
    "Once we have these we can caluclate the ROUGE-1 and ROUGE-2 precision and recall. The following is a toy example adapted from a blog post found on [www.RXNLP.com](https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.XANBwhB6ov4)\n",
    "\n",
    "Example machine generated summary = \"the dog ate all my homework\" \n",
    "Example gold standard reference model = \"the dog ate my homework\" \n",
    "\n",
    "$$\\text{ROUGE-N Recall} = \\dfrac{\\text{number of overlapping n-grams}}{\\text{number of words in gold standard}}$$\n",
    "\n",
    "$$\\text{ROUGE-N Precision} = \\dfrac{\\text{number of overlapping n-grams}}{\\text{number of words in generated summary}}$$\n",
    "\n",
    "Using our toy example to calculate ROGUE-1(unigram) precision and recall outputs the following results:\n",
    "\n",
    "$$\\text{ROUGE-1 Recall} = \\dfrac{\\text{5}}{\\text{5}} = 1.0 $$\n",
    "\n",
    "$$\\text{ROUGE-1 Precision} = \\dfrac{\\text{5}}{\\text{6}} = 0.8333... $$\n",
    "\n",
    "These results can also be obtained by using [rougescore.py](https://github.com/bdusell/rougescore/blob/master/rougescore/rougescore.py). Note that, the 3rd parameter, referred to as Î± is a number between 0 and 1 where 0 favors recall and 1 favors precision.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-1 Recall: = 1.0\n",
      "Rouge-1 Precision:  = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import rougescore\n",
    "\n",
    "summary_test = [\"the\", \"dog\", \"ate\", \"all\", \"my\", \"homework\"]\n",
    "reference_test = [[\"the\", \"dog\", \"ate\", \"my\", \"homework\"]]\n",
    "rouge1_recall_test = rougescore.rouge_1(summary_test, reference_test, 0)\n",
    "rouge1_precision_test = rougescore.rouge_1(summary_test, reference_test, 1)\n",
    "\n",
    "print(\"Rouge-1 Recall: = \" + str(rouge1_recall_test))\n",
    "print(\"Rouge-1 Precision:  = \" + str(rouge1_precision_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Results #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Precision: \n",
    "\n",
    "$$\n",
    "\\begin{array}{rr} \\hline\n",
    "    \\hline\n",
    "    & ROGUE-1 & ROGUE-2 & Avg Word Count \\\\[0.5ex]\n",
    "    \\hline\n",
    "    SumBasic & 0.03037 & 0.0777 &  34 \\\\\n",
    "    \\hline\n",
    "    Opinosis Implementation & ??? & ??? & ??? \\\\\n",
    "    \\hline\n",
    "    Opinosis Paper & 0.2831 & 0.0853 & 15 \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "### Recall: \n",
    "$$\n",
    "\\begin{array}{rr} \\hline\n",
    "    \\hline\n",
    "    & ROGUE-1 & ROGUE-2 & Avg Word Count \\\\[0.5ex]\n",
    "    \\hline\n",
    "    SumBasic & 0.1538 & 0.0377 & 34 \\\\\n",
    "    \\hline\n",
    "    Opinosis Implementation & ??? & ??? & ??? \\\\\n",
    "    \\hline\n",
    "    Research Results & 0.4482 & 0.1416 & 15 \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Semantic sorting is carried out by passing each sentence returned from the extended SumBasic algorithm into a model to predict if the sentence is positive or negative. The model was created using a Long Short Term Memory Recursive Neural Network. The training data was preprocessed by sorting a sample of yelp reviews into positive and negative categories based the star rating the review had. One and Two stars were considered a negative review. Four and five stars were considered a positive review. Then Three methods were tested for training the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Experimental Setup #2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first method was to pass the entire text of the review through the network with it's label of 'Positive' or 'Negative'. The second method was to split the text of the review into sentences and pass the sentences through the network individually with the corresponding label. The third method experimented with, passing all of the bigrams generated from each sentence as a bag of words. Method 3 was undertaken as an adaptation of FastText to a RNN and as the results show was the least effective.  \n",
    "\n",
    "The sorting method is to take the sentences produced by the summarizing algorithm then pass each of them into a model to produce two values. The sentence is sorted by the argmax of the values produced by the prediction. Model 1 is used to produce sentiment predictions. Table 3 shows the considerable increase in accuracy Model 1 has over Models 2 and 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Formating Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the input reviews by either 1 and 2 stars or 3 and 4 stars add the review text to with either a 'Positive' or 'Negative' label into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random \n",
    "import csv\n",
    "\n",
    "def basicDataFormatting():\n",
    "    dataframe = pd.read_csv('reviews.csv')\n",
    "    sample_size = 10000\n",
    "    neg_stars = [1,2]\n",
    "    pos_stars = [4,5]\n",
    "\n",
    "    pos = dataframe.loc[dataframe['stars'].isin(pos_stars)]\n",
    "    neg = dataframe.loc[dataframe['stars'].isin(neg_stars)]\n",
    "\n",
    "    pos_sample = pos.sample(sample_size)\n",
    "    neg_sample = neg.sample(sample_size)\n",
    "\n",
    "    print('Positive: ', pos_sample.size)\n",
    "    print('Negative: ', neg_sample.size)\n",
    "\n",
    "    sampled_data = []\n",
    "\n",
    "    for i, row in pos_sample.iterrows():\n",
    "        sampled_data.append(['Positive', row[\"text\"]])\n",
    "    for i, row in neg_sample.iterrows():\n",
    "        sampled_data.append(['Positive', row[\"text\"]])\n",
    "\n",
    "    random.shuffle(sampled_data)\n",
    "    outframe = pd.DataFrame(sampled_data, columns=['Sentiment', 'Text'])\n",
    "\n",
    "    outframe.to_csv(f'sentiment_yelp_data_split_{sample_size*2}_sample.csv')\n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use a nltk sentence tokenizer to split the reviews and repeat the process above to added sentence to a pandas dataframe with a 'Positive' or 'Negative' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentenceSplitDataFormatting():\n",
    "    dataframe = pd.read_csv('reviews.csv')\n",
    "    sample_size = 5000\n",
    "    neg_stars = [1,2]\n",
    "    pos_stars = [4,5]\n",
    "\n",
    "    pos = dataframe.loc[dataframe['stars'].isin(pos_stars)]\n",
    "    neg = dataframe.loc[dataframe['stars'].isin(neg_stars)]\n",
    "\n",
    "    pos_sample = pos.sample(sample_size)\n",
    "    neg_sample = neg.sample(sample_size)\n",
    "\n",
    "    print('Positive: ', pos_sample.size)\n",
    "    print('Negative: ', neg_sample.size)\n",
    "\n",
    "    sampled_data = []\n",
    "\n",
    "    for _, row in pos_sample.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        tokenized_review = sent_tokenize(review)\n",
    "        for sentence in tokenized_review:\n",
    "            sampled_data.append(['Positive', sentence])\n",
    "    for _, row in neg_sample.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        tokenized_review = sent_tokenize(review)\n",
    "        for sentence in tokenized_review:\n",
    "            sampled_data.append(['Negative', sentence])\n",
    "\n",
    "    random.shuffle(sampled_data)\n",
    "    outframe = pd.DataFrame(sampled_data, columns=['Sentiment', 'Text'])\n",
    "\n",
    "    outframe.to_csv(f'sentiment_yelp_data_sentence_split_{sample_size*2}_sample.csv')\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sort the reviews based on star level, then split each review text by sentnece, then generate all of the bigrams present in the sentence and append the bigrams to the end of the sentence before adding to a pandas dataframe with 'Positive' or 'Negative' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(sentence, n):\n",
    "    \n",
    "    if n >= len(sentence):\n",
    "        return sentence\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    ngrams = []\n",
    "    \n",
    "    for i in range(len(words)-n):\n",
    "        ngram = \"\"\n",
    "        for j in range(n):\n",
    "            ngram = f'{ngram}{\" \" + words[i+j]}'    \n",
    "        ngrams.append(ngram)\n",
    "    words = words + ngrams\n",
    "    return ' '.join(words)\n",
    "\n",
    "def ngramDataFormatting():\n",
    "    dataframe = pd.read_csv('reviews.csv')\n",
    "    sample_size = 5000\n",
    "    neg_stars = [1,2]\n",
    "    pos_stars = [4,5]\n",
    "\n",
    "    pos = dataframe.loc[dataframe['stars'].isin(pos_stars)]\n",
    "    neg = dataframe.loc[dataframe['stars'].isin(neg_stars)]\n",
    "\n",
    "    pos_sample = pos.sample(sample_size)\n",
    "    neg_sample = neg.sample(sample_size)\n",
    "\n",
    "    print('Positive: ', pos_sample.size)\n",
    "    print('Negative: ', neg_sample.size)\n",
    "\n",
    "    sampled_data = []\n",
    "\n",
    "    for i, row in pos_sample.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        tokenized_review = sent_tokenize(review)\n",
    "        for sentence in tokenized_review:\n",
    "            ngram_sentence = generate_ngrams(sentence, 2)\n",
    "            sampled_data.append(['Positive', ngram_sentence])\n",
    "    for i, row in neg_sample.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        tokenized_review = sent_tokenize(review)\n",
    "        for sentence in tokenized_review:\n",
    "            ngram_sentence = generate_ngrams(sentence, 2)\n",
    "            sampled_data.append(['Negative', ngram_sentence])\n",
    "\n",
    "    random.shuffle(sampled_data)\n",
    "    outframe = pd.DataFrame(sampled_data, columns=['Sentiment', 'Text'])\n",
    "\n",
    "    outframe.to_csv(f'sentiment_yelp_data_ngram_split_{sample_size*2}_sample.csv')\n",
    "    print('Done')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The RNN was created using keras and tensorflow libraries and the alogrithm was adapted from Peter Nagys write up on using a Long Short Term Memory RNN. The tuning variables adjusted were `max_features` and `batch_size` which were increased to 3000 and 64 respectively. The other two tuning features which were adjusted were `embed_dim` and `lstm_out` which were increased to 512 and 256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras - algorithm\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import csv\n",
    "\n",
    "data = pd.read_csv('sentiment_yelp_data_sentence_split_20000_sample.csv')\n",
    "\n",
    "max_features = 3000\n",
    "print(\"Preprocessing Text Sequences ... \")\n",
    "# Preprocess the text strings into vectors and pad the vectors to ensure consitent size \n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['Text'].values)\n",
    "text_sequences = tokenizer.texts_to_sequences(data['Text'].values)\n",
    "text_sequences = pad_sequences(text_sequences)\n",
    "\n",
    "print(\"Creating Model ...\")\n",
    "# generate the tensorflow RNN using a softmax funciton for the neruons \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 512, input_length=text_sequences.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "sentiments = pd.get_dummies(data['Sentiment']).values\n",
    "\n",
    "# Partition the input data to training and testing\n",
    "text_sequences_train, text_sequence_test, sentiments_train, sentiments_test = train_test_split(text_sequences,sentiments, test_size = 0.20, random_state = 42)\n",
    "print(text_sequences_train.shape,sentiments_train.shape)\n",
    "print(text_sequence_test.shape,sentiments_test.shape)\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "batch_size = 64\n",
    "model.fit(text_sequences_train, sentiments_train, epochs = 5, batch_size=batch_size, verbose = 1)\n",
    "\n",
    "# Validate the model on the testing partition\n",
    "validation_size = 1000\n",
    "\n",
    "sentiments_validate = sentiments_test[-validation_size:]\n",
    "text_sequence_validate = text_sequence_test[-validation_size:]\n",
    "sentiments_test = sentiments_test[:-validation_size]\n",
    "text_sequence_test = text_sequence_test[:-validation_size]\n",
    "\n",
    "# Use model.predict() to output numpy vector of size 2, [negative <float>, positive <float>]\n",
    "positive_count, positive_correct = 0, 0,\n",
    "negative_count, negative_correct = 0, 0\n",
    "for x in range(len(text_sequence_validate)):\n",
    "    \n",
    "    result = model.predict(text_sequence_validate[x].reshape(1,text_sequence_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    \n",
    "    if np.argmax(result) == np.argmax(sentiments_validate[x]):\n",
    "        if np.argmax(sentiments_validate[x]) == 0:\n",
    "            negative_correct += 1\n",
    "        else:\n",
    "            positive_correct += 1\n",
    "       \n",
    "    if np.argmax(sentiments_validate[x]) == 0:\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        positive_count += 1\n",
    "\n",
    "# print results and save the model\n",
    "print(\"Positive Accuracy: \", positive_correct/positive_count*100, \"%\")\n",
    "print(\"Negative Accuracy: \", negative_correct/negative_count*100, \"%\")\n",
    "\n",
    "model.save(\"model2.h5\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Results of the internal validation testing during training on patritioned input data:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{rr} \\hline\n",
    "    \\hline\n",
    "    Method & Positive & Negative \\\\[0.5ex]\n",
    "    \\hline\n",
    "    Model 1 & 0.944 & 0.939 \\\\\n",
    "    \\hline\n",
    "    Model 2 & 0.770 & 0.777 \\\\\n",
    "    \\hline\n",
    "    Model 3 & 0.584 & 0.833 \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analysis of the Results #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model 1 proved to be the most accurate model trained. The reason model 1 was the most successful was because the shape of the input data. Model took the whole review in and with the max features for the RNN set to 3000, the review provided more data points to assist in the prediction of positive and negative sentiment. \n",
    "\n",
    "Model 2 suffered from a lack of words in the sentence and therefore could not leverage the max features as much as Model 1. Model 3 was the least effective because adapting bigrams and the bag of words approach to a LSTM RNN resulted in a lot of repeated words per sentence. This added noise to the data being put through the network. The RNN by design considers the ngrams through the sentences history and by forcing the RNN to focus on bigrams by repeating them became problematic.\n",
    "\n",
    "5 epochs were used for training because the training time per epoch varied between 45 minuets to 2 hours. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Final Ouput "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$\n",
    "\\text{One example of output from the 252 reviews of Toronto's Eaton's Center}\\\\\n",
    "$$\n",
    "$$\n",
    "\\textbf{Positive:}\n",
    "$$\n",
    "* \"The Eaton Centre does offer some positve exceptions, even considering the national demise of its namesake several years ago.\n",
    "* The Hudson Bay store on one end is a fine department store that wears its age and history well.\n",
    "* And when you consider all the other activities that Toronto has to offer within walking and transit distance, the Eaton Centre is just another interesting and fun choice within this vibrant metropolitan area.\n",
    "* Most interesting is Trinity Square which is literally just outside the door near Sears and on the opposite side from Yonge.\n",
    "$$\n",
    "\\textbf{Negative:}\n",
    "$$\n",
    "* Even at the Mall of America, the West Edmonton Mall, and this modernly designed and spacious Eaton Centre, the shopping experience soon becomes similar to any other big mall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Future Work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Future work for the RNN:\n",
    "A possible avenue to improve the outcomes of the neural network sentiment analysis is to utilize a CNN  instead of the a RNN. A CNN can take in ngrams append to the end of each sentence rather then the whole review for training. This form of input is closer to the output of the summariztions and could prove to create better results\n",
    "\n",
    "Future work for the summarization:\n",
    "Future work for the summarization would include making Opinosis work. This would allow us to do abstractive sumamries which could provide much more informational valude then extractive method. We could also try to run sentiment analysis on the data before summarization. This would categorize the data into positive and negative. This could potential provide us better summaries. Another thing we could do is perform some feature extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
